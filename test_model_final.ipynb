{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7d5c2c",
   "metadata": {},
   "source": [
    "# Model Test Notebook - Orijinal Pipeline ile Test\n",
    "## Halil Melih AK√áA 221104091\n",
    "\n",
    "Bu notebook, eƒüitilmi≈ü ensemble modelini orijinal pipeline'ƒ± kullanarak test eder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b23e0",
   "metadata": {},
   "source": [
    "## Gerekli K√ºt√ºphaneleri Import Et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2582e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì spaCy model y√ºklendi\n",
      "\n",
      "‚úÖ T√ºm k√ºt√ºphaneler y√ºklendi!\n",
      "\n",
      "‚úÖ T√ºm k√ºt√ºphaneler y√ºklendi!\n"
     ]
    }
   ],
   "source": [
    "# ORIJINAL NOTEBOOK'TAKƒ∞ AYNI IMPORT'LAR\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# spaCy model y√ºkleme\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"‚úì spaCy model y√ºklendi\")\n",
    "except:\n",
    "    print(\"‚úó spaCy model bulunamadƒ±\")\n",
    "    nlp = None\n",
    "\n",
    "# Diƒüer gerekli k√ºt√ºphaneler\n",
    "from textstat import flesch_reading_ease, automated_readability_index\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n‚úÖ T√ºm k√ºt√ºphaneler y√ºklendi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2541202",
   "metadata": {},
   "source": [
    "## Veri Y√ºkleme ve Model Y√ºkleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c838c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úó Hi√ßbir veri dosyasƒ± bulunamadƒ±!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_source' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úó Hi√ßbir veri dosyasƒ± bulunamadƒ±!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m         exit()\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mVeri kaynaƒüƒ±: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdata_source\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVeri boyutu: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnews_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS√ºtunlar: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(news_df\u001b[38;5;241m.\u001b[39mcolumns)[:\u001b[38;5;241m10\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# ƒ∞lk 10 s√ºtun g√∂ster\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_source' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Test verilerini y√ºkle\n",
    "try:\n",
    "    news_df = pd.read_csv(\"../stockMarket_predict/Combined_News_DJIA.csv\")\n",
    "    print(f\"‚úì Combined_News_DJIA.csv y√ºklendi: {news_df.shape}\")\n",
    "    data_source = \"Combined_News_DJIA.csv\"\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        news_df = pd.read_csv(\"../stockMarket_predict/upload_DJIA_table.csv\")\n",
    "        print(f\"‚úì upload_DJIA_table.csv y√ºklendi: {news_df.shape}\")\n",
    "        data_source = \"upload_DJIA_table.csv\"\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚úó Hi√ßbir veri dosyasƒ± bulunamadƒ±!\")\n",
    "        exit()\n",
    "\n",
    "print(f\"\\nVeri kaynaƒüƒ±: {data_source}\")\n",
    "print(f\"Veri boyutu: {news_df.shape}\")\n",
    "print(f\"S√ºtunlar: {list(news_df.columns)[:10]}...\")  # ƒ∞lk 10 s√ºtun g√∂ster\n",
    "\n",
    "# Label daƒüƒ±lƒ±mƒ±nƒ± kontrol et\n",
    "if 'Label' in news_df.columns:\n",
    "    print(f\"\\nüìä Label Daƒüƒ±lƒ±mƒ±:\")\n",
    "    print(news_df['Label'].value_counts())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Label s√ºtunu bulunamadƒ±!\")\n",
    "\n",
    "# Eƒüitilmi≈ü modeli y√ºkle\n",
    "try:\n",
    "    with open(\"ensemble_model.pkl\", \"rb\") as f:\n",
    "        ensemble_model = pickle.load(f)\n",
    "    print(\"\\n‚úì Ensemble model y√ºklendi\")\n",
    "    print(f\"Model tipi: {type(ensemble_model).__name__}\")\n",
    "    if hasattr(ensemble_model, 'estimators'):\n",
    "        print(f\"Base estimators: {len(ensemble_model.estimators)}\")\n",
    "        for name, estimator in ensemble_model.estimators:\n",
    "            print(f\"  - {name}: {type(estimator).__name__}\")\n",
    "except FileNotFoundError:\n",
    "    ensemble_model = None\n",
    "    print(\"\\n‚úó Ensemble model bulunamadƒ±!\")\n",
    "    print(\"√ñnce training notebook'unu √ßalƒ±≈ütƒ±rarak modeli eƒüitin.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d17b34",
   "metadata": {},
   "source": [
    "## Orijinal Feature Engineering Fonksiyonlarƒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb5e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIJINAL NOTEBOOK'TAKƒ∞ AYNI FONKSƒ∞YONLAR\n",
    "\n",
    "def pos_features_spacy(text):\n",
    "    \"\"\"spaCy ile POS tag √∂zellikleri\"\"\"\n",
    "    if nlp is None:\n",
    "        return [0.25, 0.25, 0.25, 0.25]  # Default deƒüerler\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    total = len(doc)\n",
    "    if total == 0:\n",
    "        return [0, 0, 0, 0]\n",
    "    \n",
    "    noun_ratio = len([token for token in doc if token.pos_ == \"NOUN\"]) / total\n",
    "    verb_ratio = len([token for token in doc if token.pos_ == \"VERB\"]) / total\n",
    "    adj_ratio = len([token for token in doc if token.pos_ == \"ADJ\"]) / total\n",
    "    adv_ratio = len([token for token in doc if token.pos_ == \"ADV\"]) / total\n",
    "    return [noun_ratio, verb_ratio, adj_ratio, adv_ratio]\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Orijinal metin temizleme fonksiyonu\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"b['\\\"]|['\\\"]\", \"\", text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\d+', 'NUMBER', text)\n",
    "    text = re.sub(r'[^\\w\\s.,!?;:]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def linguistic_features(text):\n",
    "    \"\"\"Linguistik √∂zellikler √ßƒ±karma\"\"\"\n",
    "    words = text.split()\n",
    "    avg_word_len = np.mean([len(w) for w in words]) if words else 0\n",
    "    punct_count = sum([1 for c in text if c in string.punctuation])\n",
    "    cap_ratio = sum([1 for c in text if c.isupper()]) / (len(text) + 1e-9)\n",
    "    digit_ratio = sum([1 for c in text if c.isdigit()]) / (len(text) + 1e-9)\n",
    "    flesch = flesch_reading_ease(text)\n",
    "    ari = automated_readability_index(text)\n",
    "    return [len(words), avg_word_len, punct_count, cap_ratio, digit_ratio, flesch, ari]\n",
    "\n",
    "financial_keywords = [\"bull\", \"bear\", \"gain\", \"loss\", \"stock\", \"market\"]\n",
    "\n",
    "def financial_keyword_density(text):\n",
    "    \"\"\"Finansal anahtar kelime yoƒüunluƒüu\"\"\"\n",
    "    tokens = text.lower().split()\n",
    "    return [tokens.count(word)/len(tokens) if len(tokens) > 0 else 0 for word in financial_keywords]\n",
    "\n",
    "def ner_features(text):\n",
    "    \"\"\"Named Entity Recognition √∂zellikleri\"\"\"\n",
    "    if nlp is None:\n",
    "        return [0, 0, 0, 0]\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    counts = {\"PERSON\":0, \"ORG\":0, \"GPE\":0, \"MONEY\":0}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in counts:\n",
    "            counts[ent.label_] += 1\n",
    "    return list(counts.values())\n",
    "\n",
    "print(\"‚úÖ T√ºm feature engineering fonksiyonlarƒ± tanƒ±mlandƒ±!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bb77a3",
   "metadata": {},
   "source": [
    "## Veri √ñn ƒ∞≈üleme - Orijinal Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd137706",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== VERƒ∞ √ñN ƒ∞≈ûLEME - ORƒ∞Jƒ∞NAL Pƒ∞PELƒ∞NE ===\")\n",
    "\n",
    "# Orijinal notebook'taki aynƒ± preprocessing\n",
    "news_df['Combined'] = news_df.iloc[:, 2:27].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "news_df['Cleaned'] = news_df['Combined'].apply(clean_text)\n",
    "\n",
    "print(f\"‚úì {len(news_df)} √∂rnek i≈ülendi\")\n",
    "print(f\"‚úì Ortalama metin uzunluƒüu: {news_df['Cleaned'].str.len().mean():.1f} karakter\")\n",
    "\n",
    "# ƒ∞lk birka√ß √∂rneƒüi g√∂ster\n",
    "print(\"\\nüìù ƒ∞lk 3 temizlenmi≈ü metin √∂rneƒüi:\")\n",
    "for i in range(min(3, len(news_df))):\n",
    "    text = news_df['Cleaned'].iloc[i][:100] + \"...\" if len(news_df['Cleaned'].iloc[i]) > 100 else news_df['Cleaned'].iloc[i]\n",
    "    label = news_df['Label'].iloc[i] if 'Label' in news_df.columns else \"?\"\n",
    "    print(f\"  {i+1}. Label: {label} | Text: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b393fff",
   "metadata": {},
   "source": [
    "## Feature Extraction - Orijinal Sƒ±ralama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae6450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== FEATURE EXTRACTION - ORƒ∞Jƒ∞NAL SIRALAMA ===\")\n",
    "\n",
    "# Adƒ±m 2a: Linguistik √ñzellikler\n",
    "print(\"üìä Adƒ±m 2a: Linguistik √∂zellikler √ßƒ±karƒ±lƒ±yor...\")\n",
    "ling_df = pd.DataFrame(news_df['Cleaned'].apply(linguistic_features).tolist(), \n",
    "                      columns=[\"word_count\", \"avg_word_len\", \"punct_count\", \n",
    "                              \"cap_ratio\", \"digit_ratio\", \"flesch\", \"ari\"])\n",
    "print(f\"  ‚úì {ling_df.shape[1]} linguistik √∂zellik √ßƒ±karƒ±ldƒ±\")\n",
    "\n",
    "# Adƒ±m 2b: Semantik √ñzellikler\n",
    "print(\"üòä Adƒ±m 2b: Semantik √∂zellikler √ßƒ±karƒ±lƒ±yor...\")\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_df = news_df['Cleaned'].apply(lambda x: pd.Series(sia.polarity_scores(x)))\n",
    "print(f\"  ‚úì {sentiment_df.shape[1]} sentiment √∂zellik √ßƒ±karƒ±ldƒ±\")\n",
    "\n",
    "# Finansal anahtar kelime yoƒüunluƒüu (orijinal kodda hesaplandƒ± ama birle≈ütirilmedi)\n",
    "fin_kw_df = pd.DataFrame(news_df['Cleaned'].apply(financial_keyword_density).tolist(), \n",
    "                        columns=[f'kw_{k}' for k in financial_keywords])\n",
    "print(f\"  ‚úì {fin_kw_df.shape[1]} finansal anahtar kelime √∂zellik √ßƒ±karƒ±ldƒ±\")\n",
    "\n",
    "# NER √∂zellikleri (orijinal kodda hesaplandƒ± ama birle≈ütirilmedi)\n",
    "ner_df = pd.DataFrame(news_df['Cleaned'].apply(ner_features).tolist(), \n",
    "                     columns=[\"PERSON\", \"ORG\", \"GPE\", \"MONEY\"])\n",
    "print(f\"  ‚úì {ner_df.shape[1]} NER √∂zellik √ßƒ±karƒ±ldƒ±\")\n",
    "\n",
    "# Adƒ±m 2c: S√∂zdizimsel √ñzellikler (POS tag oranlarƒ±)\n",
    "print(\"üè∑Ô∏è Adƒ±m 2c: POS tag √∂zellikleri √ßƒ±karƒ±lƒ±yor...\")\n",
    "pos_df = pd.DataFrame(news_df['Cleaned'].apply(pos_features_spacy).tolist(), \n",
    "                     columns=[\"noun_ratio\", \"verb_ratio\", \"adj_ratio\", \"adv_ratio\"])\n",
    "print(f\"  ‚úì {pos_df.shape[1]} POS √∂zellik √ßƒ±karƒ±ldƒ±\")\n",
    "\n",
    "# Adƒ±m 2d: ƒ∞statistiksel √ñzellikler (TF-IDF)\n",
    "print(\"üìù Adƒ±m 2d: TF-IDF √∂zellikleri √ßƒ±karƒ±lƒ±yor...\")\n",
    "tfidf = TfidfVectorizer(max_features=2000, ngram_range=(1, 2), min_df=2, max_df=0.95, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(news_df['Cleaned'])\n",
    "pca = PCA(n_components=50)\n",
    "tfidf_pca = pca.fit_transform(tfidf_matrix.toarray())\n",
    "tfidf_df = pd.DataFrame(tfidf_pca, columns=[f'pca_{i}' for i in range(tfidf_pca.shape[1])])\n",
    "print(f\"  ‚úì {tfidf_df.shape[1]} TF-IDF+PCA √∂zellik √ßƒ±karƒ±ldƒ±\")\n",
    "\n",
    "# ORƒ∞Jƒ∞NAL SIRALAMAYA G√ñRE Bƒ∞RLE≈ûTƒ∞RME\n",
    "# Orijinal kod: features = pd.concat([ling_df, sentiment_df, pos_df, tfidf_df], axis=1)\n",
    "features = pd.concat([ling_df, sentiment_df, pos_df, tfidf_df], axis=1)\n",
    "labels = news_df['Label']\n",
    "\n",
    "print(f\"\\n‚úÖ Toplam {features.shape[1]} √∂zellik √ßƒ±karƒ±ldƒ± (orijinal sƒ±ralama ile)!\")\n",
    "print(f\"üìä Feature matrix boyutu: {features.shape}\")\n",
    "print(f\"üìã Labels boyutu: {labels.shape}\")\n",
    "\n",
    "# Feature istatistikleri\n",
    "print(\"\\nüìà Feature ƒ∞statistikleri:\")\n",
    "print(f\"  - Linguistik: {ling_df.shape[1]} √∂zellik\")\n",
    "print(f\"  - Sentiment: {sentiment_df.shape[1]} √∂zellik\")\n",
    "print(f\"  - POS Tags: {pos_df.shape[1]} √∂zellik\")\n",
    "print(f\"  - TF-IDF+PCA: {tfidf_df.shape[1]} √∂zellik\")\n",
    "print(f\"  - Toplam: {features.shape[1]} √∂zellik\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b98c368",
   "metadata": {},
   "source": [
    "## Feature Scaling ve Polynomial Features - Orijinal Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba86c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== FEATURE SCALING VE POLYNOMIAL FEATURES ===\")\n",
    "\n",
    "# Adƒ±m 5: Feature Scaling (orijinal kod)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features.values)\n",
    "print(f\"‚úì Feature scaling tamamlandƒ±: {X_scaled.shape}\")\n",
    "\n",
    "# Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "print(f\"‚úì Polynomial features olu≈üturuldu: {X_poly.shape}\")\n",
    "\n",
    "# Memory kullanƒ±mƒ± kontrol√º\n",
    "memory_usage_mb = X_poly.nbytes / (1024 * 1024)\n",
    "print(f\"üíæ Memory kullanƒ±mƒ±: {memory_usage_mb:.1f} MB\")\n",
    "\n",
    "if memory_usage_mb > 1000:\n",
    "    print(\"‚ö†Ô∏è Y√ºksek memory kullanƒ±mƒ± tespit edildi!\")\n",
    "else:\n",
    "    print(\"‚úÖ Memory kullanƒ±mƒ± kabul edilebilir seviyede\")\n",
    "\n",
    "print(f\"\\nüìä Final feature pipeline:\")\n",
    "print(f\"  Original features: {features.shape}\")\n",
    "print(f\"  After scaling: {X_scaled.shape}\")\n",
    "print(f\"  After polynomial: {X_poly.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef643c7",
   "metadata": {},
   "source": [
    "## Model Test ve Tahmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a49739",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODEL TEST VE TAHMƒ∞N ===\")\n",
    "\n",
    "if ensemble_model is None:\n",
    "    print(\"‚ùå Model y√ºklenmediƒüi i√ßin tahmin yapƒ±lamƒ±yor!\")\n",
    "    print(\"√ñnce training notebook'unu √ßalƒ±≈ütƒ±rarak modeli eƒüitin.\")\n",
    "else:\n",
    "    try:\n",
    "        # Adƒ±m 6: Train-Test Split (orijinal kod)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_poly, labels, test_size=0.2, random_state=42)\n",
    "        print(f\"‚úì Train-test split yapƒ±ldƒ±:\")\n",
    "        print(f\"  Train: {X_train.shape}\")\n",
    "        print(f\"  Test: {X_test.shape}\")\n",
    "        \n",
    "        # Model ile tahmin yap\n",
    "        print(\"\\nü§ñ Ensemble model ile tahminler yapƒ±lƒ±yor...\")\n",
    "        y_pred = ensemble_model.predict(X_test)\n",
    "        \n",
    "        # Model bilgileri\n",
    "        print(f\"\\nü§ñ MODEL Bƒ∞LGƒ∞LERƒ∞:\")\n",
    "        print(f\"  Model tipi: {type(ensemble_model).__name__}\")\n",
    "        if hasattr(ensemble_model, 'estimators'):\n",
    "            print(f\"  Base estimators: {len(ensemble_model.estimators)}\")\n",
    "            for name, estimator in ensemble_model.estimators:\n",
    "                print(f\"    - {name}: {type(estimator).__name__}\")\n",
    "        \n",
    "        # Orijinal evaluate_model fonksiyonu\n",
    "        def evaluate_model(model, X_test, y_test, y_pred):\n",
    "            \"\"\"Kapsamlƒ± model deƒüerlendirmesi\"\"\"\n",
    "            \n",
    "            print(\"\\n=== MODEL PERFORMANSI ===\")\n",
    "            print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "            print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "            print(\"\\nDetaylƒ± Rapor:\")\n",
    "            print(classification_report(y_test, y_pred))\n",
    "            \n",
    "            # Confusion Matrix\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.ylabel('Ger√ßek')\n",
    "            plt.xlabel('Tahmin')\n",
    "            plt.show()\n",
    "            \n",
    "            return accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)\n",
    "        \n",
    "        # Model deƒüerlendirmesi\n",
    "        accuracy, f1 = evaluate_model(ensemble_model, X_test, y_test, y_pred)\n",
    "        \n",
    "        print(f\"\\nüìä SONU√á √ñZETƒ∞:\")\n",
    "        print(f\"  üéØ Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        print(f\"  üéØ Test F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Baseline kar≈üƒ±la≈ütƒ±rmasƒ±\n",
    "        baseline_accuracy = max(pd.Series(y_test).value_counts(normalize=True))\n",
    "        improvement = accuracy - baseline_accuracy\n",
    "        print(f\"  üìä Baseline Accuracy: {baseline_accuracy:.4f}\")\n",
    "        print(f\"  üìà Model ƒ∞yile≈ütirmesi: +{improvement:.4f} ({improvement*100:.2f} pp)\")\n",
    "        \n",
    "        # Performans yorumu\n",
    "        if accuracy > 0.55:\n",
    "            print(f\"  ‚úÖ √áok iyi performans! (>55%)\")\n",
    "        elif accuracy > 0.50:\n",
    "            print(f\"  ‚ö†Ô∏è Kabul edilebilir performans (50-55%)\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå D√º≈ü√ºk performans (<50%)\")\n",
    "            \n",
    "        if improvement > 0.05:\n",
    "            print(f\"  ‚úÖ Baseline'dan anlamlƒ± iyile≈ütirme saƒülandƒ±!\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Baseline'dan sƒ±nƒ±rlƒ± iyile≈ütirme\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Tahmin sƒ±rasƒ±nda hata olu≈ütu: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc5024",
   "metadata": {},
   "source": [
    "## Tahmin √ñrnekleri ve Detaylƒ± Analiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0610f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ensemble_model is not None and 'y_pred' in locals():\n",
    "    print(\"=== TAHMƒ∞N √ñRNEKLERƒ∞ VE DETAYLI ANALƒ∞Z ===\")\n",
    "    \n",
    "    # Test set indekslerini al\n",
    "    test_indices = X_test.index if hasattr(X_test, 'index') else range(len(X_test))\n",
    "    \n",
    "    print(\"\\nüìù ƒ∞lk 10 Test √ñrneƒüi:\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for i in range(min(10, len(y_test))):\n",
    "        true_label = y_test.iloc[i] if hasattr(y_test, 'iloc') else y_test[i]\n",
    "        pred_label = y_pred[i]\n",
    "        \n",
    "        true_text = \"üìà UP\" if true_label == 1 else \"üìâ DOWN\"\n",
    "        pred_text = \"üìà UP\" if pred_label == 1 else \"üìâ DOWN\"\n",
    "        correct = \"‚úÖ\" if pred_label == true_label else \"‚ùå\"\n",
    "        \n",
    "        # Test set'teki ger√ßek indeksi bul\n",
    "        try:\n",
    "            if hasattr(X_test, 'index'):\n",
    "                original_idx = X_test.index[i]\n",
    "                text = news_df['Cleaned'].iloc[original_idx][:80] + \"...\" if len(news_df['Cleaned'].iloc[original_idx]) > 80 else news_df['Cleaned'].iloc[original_idx]\n",
    "            else:\n",
    "                # Basit yakla≈üƒ±m - test verilerinden g√∂ster\n",
    "                text = \"[Test set'ten metin - indeks e≈üle≈ütirilemedi]\"\n",
    "        except:\n",
    "            text = \"[Metin alƒ±namadƒ±]\"\n",
    "        \n",
    "        print(f\"{i+1:2d}. {correct} Ger√ßek: {true_text} | Tahmin: {pred_text}\")\n",
    "        if text != \"[Test set'ten metin - indeks e≈üle≈ütirilemedi]\":\n",
    "            print(f\"    Text: {text}\")\n",
    "        print()\n",
    "    \n",
    "    # Hata analizi\n",
    "    print(\"\\nüîç HATA ANALƒ∞Zƒ∞:\")\n",
    "    correct_predictions = (y_test == y_pred).sum() if hasattr(y_test, '__iter__') else sum(y_test == y_pred)\n",
    "    total_predictions = len(y_test)\n",
    "    incorrect_predictions = total_predictions - correct_predictions\n",
    "    \n",
    "    print(f\"  ‚úÖ Doƒüru tahminler: {correct_predictions}/{total_predictions}\")\n",
    "    print(f\"  ‚ùå Yanlƒ±≈ü tahminler: {incorrect_predictions}/{total_predictions}\")\n",
    "    \n",
    "    # Confusion matrix analizi\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(f\"\\nüìä CONFUSION MATRIX ANALƒ∞Zƒ∞:\")\n",
    "    print(f\"  True Negatives (doƒüru DOWN): {tn}\")\n",
    "    print(f\"  False Positives (yanlƒ±≈ü UP): {fp}\")\n",
    "    print(f\"  False Negatives (yanlƒ±≈ü DOWN): {fn}\")\n",
    "    print(f\"  True Positives (doƒüru UP): {tp}\")\n",
    "    \n",
    "    # Precision ve Recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìà DETAYLI METRƒ∞KLER:\")\n",
    "    print(f\"  Precision (UP sƒ±nƒ±fƒ±): {precision:.4f}\")\n",
    "    print(f\"  Recall (UP sƒ±nƒ±fƒ±): {recall:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model tahminleri mevcut deƒüil, detaylƒ± analiz yapƒ±lamƒ±yor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df981784",
   "metadata": {},
   "source": [
    "## Final Rapor ve Sonu√ßlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98f27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"                    Fƒ∞NAL MODEL TEST RAPORU\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä TEST VERƒ∞Sƒ∞ Bƒ∞LGƒ∞LERƒ∞:\")\n",
    "print(f\"  üìÅ Veri kaynaƒüƒ±: {data_source}\")\n",
    "print(f\"  üìà Toplam √∂rnek sayƒ±sƒ±: {len(news_df)}\")\n",
    "print(f\"  üîß √áƒ±karƒ±lan √∂zellik sayƒ±sƒ±: {features.shape[1]}\")\n",
    "print(f\"  ‚öôÔ∏è Polynomial sonrasƒ± √∂zellik sayƒ±sƒ±: {X_poly.shape[1]}\")\n",
    "\n",
    "if 'X_test' in locals():\n",
    "    print(f\"  üß™ Test set boyutu: {X_test.shape[0]} √∂rnek\")\n",
    "\n",
    "print(f\"\\nüéØ LABEL Bƒ∞LGƒ∞LERƒ∞:\")\n",
    "if 'Label' in news_df.columns:\n",
    "    label_counts = news_df['Label'].value_counts().sort_index()\n",
    "    for label, count in label_counts.items():\n",
    "        percentage = (count / len(news_df)) * 100\n",
    "        direction = \"üìâ DOWN\" if label == 0 else \"üìà UP\"\n",
    "        print(f\"  {direction}: {count} √∂rnek ({percentage:.1f}%)\")\n",
    "\n",
    "if ensemble_model is not None and 'accuracy' in locals():\n",
    "    print(f\"\\nü§ñ MODEL PERFORMANSI:\")\n",
    "    print(f\"  üéØ Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"  üéØ Test F1 Score: {f1:.4f}\")\n",
    "    print(f\"  üìä Baseline Accuracy: {baseline_accuracy:.4f}\")\n",
    "    print(f\"  üìà Model ƒ∞yile≈ütirmesi: +{improvement:.4f} ({improvement*100:.2f} pp)\")\n",
    "    \n",
    "    print(f\"\\nüèóÔ∏è MODEL Mƒ∞MARƒ∞Sƒ∞:\")\n",
    "    print(f\"  üîß Model tipi: {type(ensemble_model).__name__}\")\n",
    "    if hasattr(ensemble_model, 'estimators'):\n",
    "        print(f\"  üë• Base estimators:\")\n",
    "        for name, estimator in ensemble_model.estimators:\n",
    "            print(f\"     - {name}: {type(estimator).__name__}\")\n",
    "    \n",
    "    # Performans deƒüerlendirmesi\n",
    "    print(f\"\\nüìà PERFORMANS DEƒûERLENDƒ∞RMESƒ∞:\")\n",
    "    if accuracy > 0.55:\n",
    "        print(f\"  ‚úÖ √áOK ƒ∞Yƒ∞ PERFORMANS! (Accuracy > 55%)\")\n",
    "    elif accuracy > 0.50:\n",
    "        print(f\"  ‚ö†Ô∏è KABUL EDƒ∞LEBƒ∞Lƒ∞R PERFORMANS (50-55%)\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå D√ú≈û√úK PERFORMANS (< 50%)\")\n",
    "        \n",
    "    if improvement > 0.05:\n",
    "        print(f\"  ‚úÖ Baseline'dan ANLAMLƒ± ƒ∞Yƒ∞LE≈ûTƒ∞RME!\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Baseline'dan sƒ±nƒ±rlƒ± iyile≈ütirme\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n‚ùå MODEL TEST EDƒ∞LEMEDƒ∞\")\n",
    "    print(f\"  Model dosyasƒ± y√ºklenemedi veya tahmin yapƒ±lamadƒ±\")\n",
    "\n",
    "print(f\"\\nüí° Fƒ∞NANSAL SENTIMENT ANALƒ∞Z NOTLARI:\")\n",
    "print(f\"  ‚Ä¢ Finansal haber sentiment analizi tipik olarak %45-60 accuracy g√∂sterir\")\n",
    "print(f\"  ‚Ä¢ Haber verileri piyasa hareketlerinin sadece ~%3'√ºn√º a√ßƒ±klar\")\n",
    "print(f\"  ‚Ä¢ %53 civarƒ± accuracy finansal ML'de NORMAL ve KABUL EDƒ∞LEBƒ∞Lƒ∞R\")\n",
    "print(f\"  ‚Ä¢ √áok y√ºksek accuracy (>70%) overfitting i≈üareti olabilir\")\n",
    "print(f\"  ‚Ä¢ Bu model complex ensemble yakla≈üƒ±mƒ± ile iyi sonu√ß alƒ±yor\")\n",
    "\n",
    "print(f\"\\nüîß KULLANILAN TEKNƒ∞KLER:\")\n",
    "print(f\"  ‚Ä¢ Linguistic features (word count, readability)\")\n",
    "print(f\"  ‚Ä¢ Sentiment analysis (VADER)\")\n",
    "print(f\"  ‚Ä¢ POS tagging (spaCy)\")\n",
    "print(f\"  ‚Ä¢ TF-IDF + PCA (dimensionality reduction)\")\n",
    "print(f\"  ‚Ä¢ Polynomial features (interaction terms)\")\n",
    "print(f\"  ‚Ä¢ Ensemble learning (MLP + SVM + XGBoost + KNN)\")\n",
    "print(f\"  ‚Ä¢ Class balancing (balanced weights)\")\n",
    "\n",
    "print(f\"\\n‚è∞ Test tarihi: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üë§ Test eden: Halil Melih AK√áA (221104091)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ MODEL TEST TAMAMLANDI!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
