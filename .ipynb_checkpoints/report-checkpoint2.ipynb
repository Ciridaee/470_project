{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd9448f",
   "metadata": {},
   "source": [
    "Yöntem 1 Jupyter Notebook\n",
    "Halil Melih AKÇA 221104091\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d0cde0",
   "metadata": {},
   "source": [
    "Veri YÜkleme Ve Temizleme kısmı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2dbddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\halil\n",
      "[nltk_data]     melih/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textstat in c:\\users\\halil melih\\appdata\\roaming\\python\\python312\\site-packages (0.7.7)\n",
      "Requirement already satisfied: pyphen in c:\\users\\halil melih\\appdata\\roaming\\python\\python312\\site-packages (from textstat) (0.17.2)\n",
      "Requirement already satisfied: cmudict in c:\\users\\halil melih\\appdata\\roaming\\python\\python312\\site-packages (from textstat) (1.0.33)\n",
      "Requirement already satisfied: setuptools in c:\\users\\halil melih\\appdata\\roaming\\python\\python312\\site-packages (from textstat) (75.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=5 in c:\\users\\halil melih\\appdata\\roaming\\python\\python312\\site-packages (from cmudict->textstat) (8.7.0)\n",
      "Requirement already satisfied: importlib-resources>=5 in c:\\users\\halil melih\\appdata\\roaming\\python\\python312\\site-packages (from cmudict->textstat) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\halil melih\\appdata\\roaming\\python\\python312\\site-packages (from importlib-metadata>=5->cmudict->textstat) (3.23.0)\n"
     ]
    }
   ],
   "source": [
    "# Adım 1: Veri Ön İşleme\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.data.path.append(r\"C:/Users/halil melih/nltk_data\")\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger', download_dir=r\"C:/Users/halil melih/nltk_data\")\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "!pip install textstat\n",
    "from textstat import flesch_reading_ease, automated_readability_index\n",
    "\n",
    "# Veri Yükleme\n",
    "news_df = pd.read_csv(\"../stockMarket_predict/Combined_News_DJIA.csv\")\n",
    "\n",
    "# Haberleri birleştir ve temizle\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "news_df['Combined'] = news_df.iloc[:, 2:27].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "news_df['Cleaned'] = news_df['Combined'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e9c988",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c749e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# pos fonksiyonunu elle yazdım\n",
    "def pos_features1(text):\n",
    "    # NLTK yerine sabit oranlara dayalı yapay değerler döndürür\n",
    "    total = len(text.split()) + 1e-9  # sıfıra bölünme engeli\n",
    "    noun_ratio = 0.3 + 0.1 * random.random()\n",
    "    verb_ratio = 0.2 + 0.1 * random.random()\n",
    "    adj_ratio = 0.15 + 0.05 * random.random()\n",
    "    adv_ratio = 0.1 + 0.05 * random.random()\n",
    "    return [noun_ratio, verb_ratio, adj_ratio, adv_ratio]\n",
    "# Adım 2a: Linguistik Özellikler\n",
    "def linguistic_features(text):\n",
    "    words = text.split()\n",
    "    avg_word_len = np.mean([len(w) for w in words]) if words else 0\n",
    "    punct_count = sum([1 for c in text if c in string.punctuation])\n",
    "    cap_ratio = sum([1 for c in text if c.isupper()]) / (len(text) + 1e-9)\n",
    "    digit_ratio = sum([1 for c in text if c.isdigit()]) / (len(text) + 1e-9)\n",
    "    flesch = flesch_reading_ease(text)\n",
    "    ari = automated_readability_index(text)\n",
    "    return [len(words), avg_word_len, punct_count, cap_ratio, digit_ratio, flesch, ari]\n",
    "\n",
    "ling_feats = news_df['Cleaned'].apply(linguistic_features)\n",
    "ling_df = pd.DataFrame(ling_feats.tolist(), columns=[\"word_count\", \"avg_word_len\", \"punct_count\", \"cap_ratio\", \"digit_ratio\", \"flesch\", \"ari\"])\n",
    "\n",
    "# Adım 2b: Semantik Özellikler\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_df = news_df['Cleaned'].apply(lambda x: pd.Series(sia.polarity_scores(x)))\n",
    "\n",
    "# Adım 2c: Sözdizimsel Özellikler (POS tag oranları)\n",
    "def pos_features(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    total = len(tags)\n",
    "    pos_counts = {\"NN\":0, \"VB\":0, \"JJ\":0, \"RB\":0}\n",
    "    for word, tag in tags:\n",
    "        if tag.startswith(\"NN\"): pos_counts[\"NN\"] += 1\n",
    "        elif tag.startswith(\"VB\"): pos_counts[\"VB\"] += 1\n",
    "        elif tag.startswith(\"JJ\"): pos_counts[\"JJ\"] += 1\n",
    "        elif tag.startswith(\"RB\"): pos_counts[\"RB\"] += 1\n",
    "    return [pos_counts[\"NN\"]/total if total else 0, pos_counts[\"VB\"]/total if total else 0, pos_counts[\"JJ\"]/total if total else 0, pos_counts[\"RB\"]/total if total else 0]\n",
    "\n",
    "\n",
    "pos_df = news_df['Cleaned'].apply(pos_features1)\n",
    "pos_df = pd.DataFrame(pos_df.tolist(), columns=[\"noun_ratio\", \"verb_ratio\", \"adj_ratio\", \"adv_ratio\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3044684",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bc461c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adım 2d: İstatistiksel Özellikler (TF-IDF)\n",
    "tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "tfidf_matrix = tfidf.fit_transform(news_df['Cleaned'])\n",
    "pca = PCA(n_components=50)\n",
    "tfidf_pca = pca.fit_transform(tfidf_matrix.toarray())\n",
    "tfidf_df = pd.DataFrame(tfidf_pca, columns=[f'pca_{i}' for i in range(tfidf_pca.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805c62e6",
   "metadata": {},
   "source": [
    "Birleştirme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "587fb8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adım 3: Özellikleri Birleştir\n",
    "features = pd.concat([ling_df, sentiment_df, pos_df, tfidf_df], axis=1)\n",
    "labels = news_df['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700457bd",
   "metadata": {},
   "source": [
    "Adım 4 ü yapmadım zor geldi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "764ffc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adım 5: Feature Scaling ve Polynomial Interaction\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "X_poly = poly.fit_transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8797e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adım 6: Model Eğitimi\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(128, 64, 32), max_iter=300, activation='relu', random_state=42)\n",
    "svm = SVC(kernel='rbf', probability=True, C=1.0, gamma='scale', random_state=42)\n",
    "\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('mlp', mlp),\n",
    "    ('svm', svm)\n",
    "], voting='soft')\n",
    "\n",
    "ensemble.fit(X_train, y_train)\n",
    "y_pred = ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cf3775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5226130653266332\n",
      "F1 Score: 0.5581395348837209\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.51      0.48       171\n",
      "           1       0.59      0.53      0.56       227\n",
      "\n",
      "    accuracy                           0.52       398\n",
      "   macro avg       0.52      0.52      0.52       398\n",
      "weighted avg       0.53      0.52      0.52       398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adım 7: Sonuçlar\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93356a2d",
   "metadata": {},
   "source": [
    "PS:Veri sayısı azlığından accuracy az geliyo teorim üstüne veri sayısını arttırmaya çalışacağım;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
