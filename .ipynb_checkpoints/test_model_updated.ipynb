{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1b907d",
   "metadata": {},
   "source": [
    "# Model Test Notebook - Orijinal Pipeline ile Test\n",
    "## Halil Melih AKÇA 221104091\n",
    "\n",
    "Bu notebook, eğitilmiş ensemble modelini orijinal pipeline'ı kullanarak test eder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c3628a",
   "metadata": {},
   "source": [
    "## Gerekli Kütüphaneleri Import Et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fc76c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " spaCy model yüklendi\n",
      "\n",
      " Tüm kütüphaneler yüklendi!\n"
     ]
    }
   ],
   "source": [
    "# ORIJINAL NOTEBOOK'TAKİ AYNI IMPORT'LAR\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# spaCy model yükleme\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\" spaCy model yüklendi\")\n",
    "except:\n",
    "    print(\" spaCy model bulunamadı\")\n",
    "    nlp = None\n",
    "\n",
    "# Diğer gerekli kütüphaneler\n",
    "from textstat import flesch_reading_ease, automated_readability_index\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n Tüm kütüphaneler yüklendi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ee29e",
   "metadata": {},
   "source": [
    "## Veri Yükleme ve Model Yükleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e00e1275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Combined_News_DJIA.csv yüklendi: (1989, 27)\n",
      "\n",
      "Veri kaynağı: Combined_News_DJIA.csv\n",
      "Veri boyutu: (1989, 27)\n",
      "Sütunlar: ['Date', 'Label', 'Top1', 'Top2', 'Top3', 'Top4', 'Top5', 'Top6', 'Top7', 'Top8']...\n",
      "\n",
      " Label Dağılımı:\n",
      "Label\n",
      "1    1065\n",
      "0     924\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Ensemble model yüklendi\n",
      " scaler yüklendi\n",
      " poly yüklendi\n",
      " pca yüklendi\n",
      " tfidf yüklendi\n",
      "\n",
      " Yüklenen bileşenler: 5/5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    news_df = pd.read_csv(\"../stockMarket_predict/Combined_News_DJIA.csv\")\n",
    "    print(f\" Combined_News_DJIA.csv yüklendi: {news_df.shape}\")\n",
    "    data_source = \"Combined_News_DJIA.csv\"\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        news_df = pd.read_csv(\"../stockMarket_predict/upload_DJIA_table.csv\")\n",
    "        print(f\" upload_DJIA_table.csv yüklendi: {news_df.shape}\")\n",
    "        data_source = \"upload_DJIA_table.csv\"\n",
    "    except FileNotFoundError:\n",
    "        print(\" Hiçbir veri dosyası bulunamadı!\")\n",
    "        exit()\n",
    "\n",
    "print(f\"\\nVeri kaynağı: {data_source}\")\n",
    "print(f\"Veri boyutu: {news_df.shape}\")\n",
    "print(f\"Sütunlar: {list(news_df.columns)[:10]}...\")  \n",
    "\n",
    "if 'Label' in news_df.columns:\n",
    "    print(f\"\\n Label Dağılımı:\")\n",
    "    print(news_df['Label'].value_counts())\n",
    "else:\n",
    "    print(\" Label sütunu bulunamadı!\")\n",
    "\n",
    "models_loaded = {}\n",
    "try:\n",
    "    with open(\"ensemble_model.pkl\", \"rb\") as f:\n",
    "        ensemble_model = pickle.load(f)\n",
    "    models_loaded['ensemble'] = True\n",
    "    print(\"\\n Ensemble model yüklendi\")\n",
    "except FileNotFoundError:\n",
    "    ensemble_model = None\n",
    "    models_loaded['ensemble'] = False\n",
    "    print(\"\\n Ensemble model bulunamadı!\")\n",
    "\n",
    "preprocessors = ['scaler', 'poly', 'pca', 'tfidf']\n",
    "loaded_preprocessors = {}\n",
    "\n",
    "for prep in preprocessors:\n",
    "    try:\n",
    "        with open(f\"{prep}.pkl\", \"rb\") as f:\n",
    "            loaded_preprocessors[prep] = pickle.load(f)\n",
    "        models_loaded[prep] = True\n",
    "        print(f\" {prep} yüklendi\")\n",
    "    except FileNotFoundError:\n",
    "        loaded_preprocessors[prep] = None\n",
    "        models_loaded[prep] = False\n",
    "        print(f\" {prep} bulunamadı\")\n",
    "\n",
    "print(f\"\\n Yüklenen bileşenler: {sum(models_loaded.values())}/{len(models_loaded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26708670",
   "metadata": {},
   "source": [
    "## Orijinal Feature Engineering Fonksiyonları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be837338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tüm feature engineering fonksiyonları tanımlandı!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def pos_features_spacy(text):\n",
    "    \"\"\"spaCy ile POS tag özellikleri\"\"\"\n",
    "    if nlp is None:\n",
    "        return [0.25, 0.25, 0.25, 0.25]  \n",
    "    doc = nlp(text)\n",
    "    total = len(doc)\n",
    "    if total == 0:\n",
    "        return [0, 0, 0, 0]\n",
    "    \n",
    "    noun_ratio = len([token for token in doc if token.pos_ == \"NOUN\"]) / total\n",
    "    verb_ratio = len([token for token in doc if token.pos_ == \"VERB\"]) / total\n",
    "    adj_ratio = len([token for token in doc if token.pos_ == \"ADJ\"]) / total\n",
    "    adv_ratio = len([token for token in doc if token.pos_ == \"ADV\"]) / total\n",
    "    return [noun_ratio, verb_ratio, adj_ratio, adv_ratio]\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Orijinal metin temizleme fonksiyonu\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"b['\\\"]|['\\\"]\", \"\", text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\d+', 'NUMBER', text)\n",
    "    text = re.sub(r'[^\\w\\s.,!?;:]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def linguistic_features(text):\n",
    "    \"\"\"Linguistik özellikler çıkarma\"\"\"\n",
    "    words = text.split()\n",
    "    avg_word_len = np.mean([len(w) for w in words]) if words else 0\n",
    "    punct_count = sum([1 for c in text if c in string.punctuation])\n",
    "    cap_ratio = sum([1 for c in text if c.isupper()]) / (len(text) + 1e-9)\n",
    "    digit_ratio = sum([1 for c in text if c.isdigit()]) / (len(text) + 1e-9)\n",
    "    flesch = flesch_reading_ease(text)\n",
    "    ari = automated_readability_index(text)\n",
    "    return [len(words), avg_word_len, punct_count, cap_ratio, digit_ratio, flesch, ari]\n",
    "\n",
    "financial_keywords = [\"bull\", \"bear\", \"gain\", \"loss\", \"stock\", \"market\"]\n",
    "\n",
    "def financial_keyword_density(text):\n",
    "    \"\"\"Finansal anahtar kelime yoğunluğu\"\"\"\n",
    "    tokens = text.lower().split()\n",
    "    return [tokens.count(word)/len(tokens) if len(tokens) > 0 else 0 for word in financial_keywords]\n",
    "\n",
    "def ner_features(text):\n",
    "    \"\"\"Named Entity Recognition özellikleri\"\"\"\n",
    "    if nlp is None:\n",
    "        return [0, 0, 0, 0]\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    counts = {\"PERSON\":0, \"ORG\":0, \"GPE\":0, \"MONEY\":0}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in counts:\n",
    "            counts[ent.label_] += 1\n",
    "    return list(counts.values())\n",
    "\n",
    "def pos_features(text):\n",
    "    \"\"\"NLTK ile POS tag özellikleri (orijinal implementation)\"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    total = len(tags)\n",
    "    pos_counts = {\"NN\":0, \"VB\":0, \"JJ\":0, \"RB\":0}\n",
    "    for _, tag in tags:\n",
    "        if tag.startswith(\"NN\"): pos_counts[\"NN\"] += 1\n",
    "        elif tag.startswith(\"VB\"): pos_counts[\"VB\"] += 1\n",
    "        elif tag.startswith(\"JJ\"): pos_counts[\"JJ\"] += 1\n",
    "        elif tag.startswith(\"RB\"): pos_counts[\"RB\"] += 1\n",
    "    return [pos_counts[\"NN\"]/total if total else 0, pos_counts[\"VB\"]/total if total else 0, pos_counts[\"JJ\"]/total if total else 0, pos_counts[\"RB\"]/total if total else 0]\n",
    "\n",
    "print(\" Tüm feature engineering fonksiyonları tanımlandı!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc2af3b",
   "metadata": {},
   "source": [
    "## Veri Ön İşleme - Orijinal Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c9abb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERİ ÖN İŞLEME - ORİJİNAL PİPELİNE ===\n",
      "✓ 1989 örnek işlendi\n",
      "✓ Ortalama metin uzunluğu: 2764.4 karakter\n",
      "\n",
      " İlk 3 temizlenmiş metin örneği:\n",
      "  1. Label: 0 | Text: georgia downs two russian warplanes as countries move to brink of war breaking: musharraf to be impe...\n",
      "  2. Label: 1 | Text: why wont america and nato help us? if they wont help us now, why did we help them in iraq? bush puts...\n",
      "  3. Label: 0 | Text: remember that adorable NUMBERyearold who sang at the opening ceremonies? that was fake, too. russia ...\n"
     ]
    }
   ],
   "source": [
    "print(\"=== VERİ ÖN İŞLEME - ORİJİNAL PİPELİNE ===\")\n",
    "\n",
    "news_df['Combined'] = news_df.iloc[:, 2:27].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "news_df['Cleaned'] = news_df['Combined'].apply(clean_text)\n",
    "\n",
    "print(f\"✓ {len(news_df)} örnek işlendi\")\n",
    "print(f\"✓ Ortalama metin uzunluğu: {news_df['Cleaned'].str.len().mean():.1f} karakter\")\n",
    "\n",
    "print(\"\\n İlk 3 temizlenmiş metin örneği:\")\n",
    "for i in range(min(3, len(news_df))):\n",
    "    text = news_df['Cleaned'].iloc[i][:100] + \"...\" if len(news_df['Cleaned'].iloc[i]) > 100 else news_df['Cleaned'].iloc[i]\n",
    "    label = news_df['Label'].iloc[i] if 'Label' in news_df.columns else \"?\"\n",
    "    print(f\"  {i+1}. Label: {label} | Text: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530582c4",
   "metadata": {},
   "source": [
    "## Feature Extraction - Orijinal Sıralama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daf51ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE EXTRACTION - ORİJİNAL SIRALAMA ===\n",
      " Adım 2a: Linguistik özellikler çıkarılıyor...\n",
      "   7 linguistik özellik çıkarıldı\n",
      " Adım 2b: Semantik özellikler çıkarılıyor...\n",
      "   4 sentiment özellik çıkarıldı\n",
      "   6 finansal anahtar kelime özellik çıkarıldı\n",
      "   4 NER özellik çıkarıldı\n",
      " Adım 2c: POS tag özellikleri çıkarılıyor...\n",
      "  ✓ 4 POS özellik çıkarıldı\n",
      " Adım 2d: TF-IDF özellikleri çıkarılıyor...\n",
      "   50 TF-IDF+PCA özellik çıkarıldı\n",
      "\n",
      " Toplam 65 özellik çıkarıldı (orijinal sıralama ile)!\n",
      " Feature matrix boyutu: (1989, 65)\n",
      " Labels boyutu: (1989,)\n",
      "\n",
      " Feature İstatistikleri:\n",
      "  - Linguistik: 7 özellik\n",
      "  - Sentiment: 4 özellik\n",
      "  - POS Tags: 4 özellik\n",
      "  - TF-IDF+PCA: 50 özellik\n",
      "  - Toplam: 65 özellik\n"
     ]
    }
   ],
   "source": [
    "print(\"=== FEATURE EXTRACTION - ORİJİNAL SIRALAMA ===\")\n",
    "\n",
    "print(\" Adım 2a: Linguistik özellikler çıkarılıyor...\")\n",
    "ling_df = pd.DataFrame(news_df['Cleaned'].apply(linguistic_features).tolist(), \n",
    "                      columns=[\"word_count\", \"avg_word_len\", \"punct_count\", \n",
    "                              \"cap_ratio\", \"digit_ratio\", \"flesch\", \"ari\"])\n",
    "print(f\"   {ling_df.shape[1]} linguistik özellik çıkarıldı\")\n",
    "\n",
    "print(\" Adım 2b: Semantik özellikler çıkarılıyor...\")\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_df = news_df['Cleaned'].apply(lambda x: pd.Series(sia.polarity_scores(x)))\n",
    "print(f\"   {sentiment_df.shape[1]} sentiment özellik çıkarıldı\")\n",
    "\n",
    "fin_kw_df = pd.DataFrame(news_df['Cleaned'].apply(financial_keyword_density).tolist(), \n",
    "                        columns=[f'kw_{k}' for k in financial_keywords])\n",
    "print(f\"   {fin_kw_df.shape[1]} finansal anahtar kelime özellik çıkarıldı\")\n",
    "\n",
    "ner_df = pd.DataFrame(news_df['Cleaned'].apply(ner_features).tolist(), \n",
    "                     columns=[\"PERSON\", \"ORG\", \"GPE\", \"MONEY\"])\n",
    "print(f\"   {ner_df.shape[1]} NER özellik çıkarıldı\")\n",
    "\n",
    "print(\" Adım 2c: POS tag özellikleri çıkarılıyor...\")\n",
    "pos_df = pd.DataFrame(news_df['Cleaned'].apply(pos_features_spacy).tolist(), \n",
    "                     columns=[\"noun_ratio\", \"verb_ratio\", \"adj_ratio\", \"adv_ratio\"])\n",
    "print(f\"  ✓ {pos_df.shape[1]} POS özellik çıkarıldı\")\n",
    "\n",
    "print(\" Adım 2d: TF-IDF özellikleri çıkarılıyor...\")\n",
    "tfidf = TfidfVectorizer(max_features=2000, ngram_range=(1, 2), min_df=2, max_df=0.95, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(news_df['Cleaned'])\n",
    "pca = PCA(n_components=50)\n",
    "tfidf_pca = pca.fit_transform(tfidf_matrix.toarray())\n",
    "tfidf_df = pd.DataFrame(tfidf_pca, columns=[f'pca_{i}' for i in range(tfidf_pca.shape[1])])\n",
    "print(f\"   {tfidf_df.shape[1]} TF-IDF+PCA özellik çıkarıldı\")\n",
    "\n",
    "features = pd.concat([ling_df, sentiment_df, pos_df, tfidf_df], axis=1)\n",
    "labels = news_df['Label']\n",
    "\n",
    "print(f\"\\n Toplam {features.shape[1]} özellik çıkarıldı (orijinal sıralama ile)!\")\n",
    "print(f\" Feature matrix boyutu: {features.shape}\")\n",
    "print(f\" Labels boyutu: {labels.shape}\")\n",
    "\n",
    "print(\"\\n Feature İstatistikleri:\")\n",
    "print(f\"  - Linguistik: {ling_df.shape[1]} özellik\")\n",
    "print(f\"  - Sentiment: {sentiment_df.shape[1]} özellik\")\n",
    "print(f\"  - POS Tags: {pos_df.shape[1]} özellik\")\n",
    "print(f\"  - TF-IDF+PCA: {tfidf_df.shape[1]} özellik\")\n",
    "print(f\"  - Toplam: {features.shape[1]} özellik\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32643f73",
   "metadata": {},
   "source": [
    "## Feature Scaling ve Polynomial Features - Orijinal Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8f794cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE SCALING VE POLYNOMIAL FEATURES ===\n",
      " Feature scaling tamamlandı: (1989, 65)\n",
      " Polynomial features oluşturuldu: (1989, 2146)\n",
      " Memory kullanımı: 32.6 MB\n",
      " Memory kullanımı kabul edilebilir seviyede\n",
      "\n",
      " Final feature pipeline:\n",
      "  Original features: (1989, 65)\n",
      "  After scaling: (1989, 65)\n",
      "  After polynomial: (1989, 2146)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== FEATURE SCALING VE POLYNOMIAL FEATURES ===\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features.values)\n",
    "print(f\" Feature scaling tamamlandı: {X_scaled.shape}\")\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "print(f\" Polynomial features oluşturuldu: {X_poly.shape}\")\n",
    "\n",
    "memory_usage_mb = X_poly.nbytes / (1024 * 1024)\n",
    "print(f\" Memory kullanımı: {memory_usage_mb:.1f} MB\")\n",
    "\n",
    "if memory_usage_mb > 1000:\n",
    "    print(\" Yüksek memory kullanımı tespit edildi!\")\n",
    "else:\n",
    "    print(\" Memory kullanımı kabul edilebilir seviyede\")\n",
    "\n",
    "print(f\"\\n Final feature pipeline:\")\n",
    "print(f\"  Original features: {features.shape}\")\n",
    "print(f\"  After scaling: {X_scaled.shape}\")\n",
    "print(f\"  After polynomial: {X_poly.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c253ff2d",
   "metadata": {},
   "source": [
    "## Model Test ve Tahmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731a29c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL TEST VE TAHMİN ===\n",
      "✓ Train-test split yapıldı:\n",
      "  Train: (1591, 2146)\n",
      "  Test: (398, 2146)\n",
      "\n",
      " Ensemble model ile tahminler yapılıyor...\n",
      " Tahmin sırasında hata oluştu: X has 2146 features, but MLPClassifier is expecting 2851 features as input.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\halil melih\\AppData\\Local\\Temp\\ipykernel_18404\\102267486.py\", line 14, in <module>\n",
      "    y_pred = ensemble_model.predict(X_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\halil melih\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\ensemble\\_voting.py\", line 422, in predict\n",
      "    maj = np.argmax(self.predict_proba(X), axis=1)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\halil melih\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\ensemble\\_voting.py\", line 463, in predict_proba\n",
      "    self._collect_probas(X), axis=0, weights=self._weights_not_none\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\halil melih\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\ensemble\\_voting.py\", line 438, in _collect_probas\n",
      "    return np.asarray([clf.predict_proba(X) for clf in self.estimators_])\n",
      "                       ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\halil melih\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 1365, in predict_proba\n",
      "    y_pred = self._forward_pass_fast(X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\halil melih\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 204, in _forward_pass_fast\n",
      "    X = validate_data(self, X, accept_sparse=[\"csr\", \"csc\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\halil melih\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py\", line 2975, in validate_data\n",
      "    _check_n_features(_estimator, X, reset=reset)\n",
      "  File \"C:\\Users\\halil melih\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py\", line 2839, in _check_n_features\n",
      "    raise ValueError(\n",
      "ValueError: X has 2146 features, but MLPClassifier is expecting 2851 features as input.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== MODEL TEST VE TAHMİN ===\")\n",
    "\n",
    "if ensemble_model is None:\n",
    "    print(\" Model yüklenmediği için tahmin yapılamıyor!\")\n",
    "    print(\"Önce training notebook'unu çalıştırarak modeli eğitin.\")\n",
    "else:\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_poly, labels, test_size=0.2, random_state=42)\n",
    "        print(f\" Train-test split yapıldı:\")\n",
    "        print(f\"  Train: {X_train.shape}\")\n",
    "        print(f\"  Test: {X_test.shape}\")\n",
    "        \n",
    "        print(\"\\n Ensemble model ile tahminler yapılıyor...\")\n",
    "        y_pred = ensemble_model.predict(X_test)\n",
    "        \n",
    "        print(f\"\\n MODEL BİLGİLERİ:\")\n",
    "        print(f\"  Model tipi: {type(ensemble_model).__name__}\")\n",
    "        if hasattr(ensemble_model, 'estimators'):\n",
    "            print(f\"  Base estimators: {len(ensemble_model.estimators)}\")\n",
    "            for name, estimator in ensemble_model.estimators:\n",
    "                print(f\"    - {name}: {type(estimator).__name__}\")\n",
    "        \n",
    "        def evaluate_model(model, X_test, y_test, y_pred):\n",
    "            \"\"\"Kapsamlı model değerlendirmesi\"\"\"\n",
    "            \n",
    "            print(\"\\n=== MODEL PERFORMANSI ===\")\n",
    "            print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "            print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "            print(\"\\nDetaylı Rapor:\")\n",
    "            print(classification_report(y_test, y_pred))\n",
    "            \n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.ylabel('Gerçek')\n",
    "            plt.xlabel('Tahmin')\n",
    "            plt.show()\n",
    "            \n",
    "            return accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)\n",
    "        \n",
    "        accuracy, f1 = evaluate_model(ensemble_model, X_test, y_test, y_pred)\n",
    "        \n",
    "        print(f\"\\n SONUÇ ÖZETİ:\")\n",
    "        print(f\"   Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        print(f\"   Test F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        baseline_accuracy = max(pd.Series(y_test).value_counts(normalize=True))\n",
    "        improvement = accuracy - baseline_accuracy\n",
    "        print(f\"  Baseline Accuracy: {baseline_accuracy:.4f}\")\n",
    "        print(f\"  Model İyileştirmesi: +{improvement:.4f} ({improvement*100:.2f} pp)\")\n",
    "        \n",
    "        if accuracy > 0.55:\n",
    "            print(f\"   Çok iyi performans! (>55%)\")\n",
    "        elif accuracy > 0.50:\n",
    "            print(f\"   Kabul edilebilir performans (50-55%)\")\n",
    "        else:\n",
    "            print(f\"   Düşük performans (<50%)\")\n",
    "            \n",
    "        if improvement > 0.05:\n",
    "            print(f\"   Baseline'dan anlamlı iyileştirme sağlandı!\")\n",
    "        else:\n",
    "            print(f\"   Baseline'dan sınırlı iyileştirme\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Tahmin sırasında hata oluştu: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
